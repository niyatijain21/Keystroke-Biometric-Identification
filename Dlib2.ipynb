{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CODE 1: LANDMARK POINTS, GAUSSIAN BLUR AND EDGE DETECTION\n",
    "https://github.com/siddh30/Glasses-Detection/blob/main/Glasses_detection.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images with glasses detected: 6740\n"
     ]
    }
   ],
   "source": [
    "import dlib\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "lfw_dataset_path = \"/Users/nsjain/Downloads/lfw\"\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"/Users/nsjain/Downloads/shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "def glasses_detector(image_path):\n",
    "    img = dlib.load_rgb_image(image_path)\n",
    "    \n",
    "    if len(detector(img)) == 0:\n",
    "        return 'No face detected'\n",
    "    \n",
    "    rect = detector(img)[0]\n",
    "    sp = predictor(img, rect)\n",
    "    landmarks = np.array([[p.x, p.y] for p in sp.parts()])\n",
    "    \n",
    "    nose_bridge_x = []\n",
    "    nose_bridge_y = []\n",
    "    \n",
    "    for i in [28, 29, 30, 31, 33, 34, 35]:\n",
    "        nose_bridge_x.append(landmarks[i][0])\n",
    "        nose_bridge_y.append(landmarks[i][1])\n",
    "    \n",
    "    x_min = min(nose_bridge_x)\n",
    "    x_max = max(nose_bridge_x)\n",
    "    \n",
    "    y_min = landmarks[20][1]\n",
    "    y_max = landmarks[30][1]\n",
    "    \n",
    "    img2 = cv2.imread(image_path)\n",
    "    img2 = img2[y_min:y_max, x_min:x_max]\n",
    "    \n",
    "    img_blur = cv2.GaussianBlur(img2, (3, 3), sigmaX=0, sigmaY=0)\n",
    "    \n",
    "    edges = cv2.Canny(image=img_blur, threshold1=100, threshold2=200)\n",
    "    \n",
    "    edges_center = edges.T[(int(len(edges.T)/2))]\n",
    "    \n",
    "    if 255 in edges_center:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "glasses_detected_count = 0\n",
    "for root, dirs, files in os.walk(lfw_dataset_path):\n",
    "    for file in files:\n",
    "        if file.endswith(\".jpg\"):\n",
    "            image_path = os.path.join(root, file)\n",
    "            result = glasses_detector(image_path)\n",
    "            \n",
    "            if result == 1:\n",
    "                glasses_detected_count += 1\n",
    "            \n",
    "print(f\"Total images with glasses detected: {glasses_detected_count}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CODE 2: LANDMARK POINTS, ALIGNMENT, GAUSSIAN BLUR AND EDGE DETECTION\n",
    "https://github.com/TianxingWu/realtime-glasses-detection/blob/master/eyeglass_detector.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/34/j8dqnjr14_7d5fwpn4wx4zm80000gq/T/ipykernel_33606/3458529870.py:50: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  scale = desired_dist / dist\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images with glasses detected: 283\n"
     ]
    }
   ],
   "source": [
    "import dlib\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "lfw_dataset_path = \"/Users/nsjain/Desktop/noglasses\"\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"/Users/nsjain/Downloads/shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "def landmarks_to_np(landmarks, dtype=\"int\"):\n",
    "    num = landmarks.num_parts\n",
    "    coords = np.zeros((num, 2), dtype=dtype)\n",
    "    for i in range(0, num):\n",
    "        coords[i] = (landmarks.part(i).x, landmarks.part(i).y)\n",
    "    return coords\n",
    "\n",
    "def get_centers(img, landmarks):\n",
    "    EYE_LEFT_OUTTER = landmarks[2]\n",
    "    EYE_LEFT_INNER = landmarks[3]\n",
    "    EYE_RIGHT_OUTTER = landmarks[0]\n",
    "    EYE_RIGHT_INNER = landmarks[1]\n",
    "\n",
    "    x = ((landmarks[0:4]).T)[0]\n",
    "    y = ((landmarks[0:4]).T)[1]\n",
    "    A = np.vstack([x, np.ones(len(x))]).T\n",
    "    k, b = np.linalg.lstsq(A, y, rcond=None)[0]\n",
    "\n",
    "    x_left = (EYE_LEFT_OUTTER[0]+EYE_LEFT_INNER[0])/2\n",
    "    x_right = (EYE_RIGHT_OUTTER[0]+EYE_RIGHT_INNER[0])/2\n",
    "    LEFT_EYE_CENTER =  np.array([np.int32(x_left), np.int32(x_left*k+b)])\n",
    "    RIGHT_EYE_CENTER =  np.array([np.int32(x_right), np.int32(x_right*k+b)])\n",
    "\n",
    "    pts = np.vstack((LEFT_EYE_CENTER,RIGHT_EYE_CENTER))\n",
    "    cv2.polylines(img, [pts], False, (255,0,0), 1)\n",
    "    cv2.circle(img, (LEFT_EYE_CENTER[0],LEFT_EYE_CENTER[1]), 3, (0, 0, 255), -1)\n",
    "    cv2.circle(img, (RIGHT_EYE_CENTER[0],RIGHT_EYE_CENTER[1]), 3, (0, 0, 255), -1)\n",
    "\n",
    "    return LEFT_EYE_CENTER, RIGHT_EYE_CENTER\n",
    "\n",
    "def get_aligned_face(img, left, right):\n",
    "    desired_w = 256\n",
    "    desired_h = 256\n",
    "    desired_dist = desired_w * 0.5\n",
    "\n",
    "    eyescenter = ((left[0]+right[0])*0.5 , (left[1]+right[1])*0.5)\n",
    "    dx = right[0] - left[0]\n",
    "    dy = right[1] - left[1]\n",
    "    dist = np.sqrt(dx*dx + dy*dy)\n",
    "    scale = desired_dist / dist\n",
    "    angle = np.degrees(np.arctan2(dy,dx))\n",
    "    M = cv2.getRotationMatrix2D(eyescenter,angle,scale)\n",
    "\n",
    "    tX = desired_w * 0.5\n",
    "    tY = desired_h * 0.5\n",
    "    M[0, 2] += (tX - eyescenter[0])\n",
    "    M[1, 2] += (tY - eyescenter[1])\n",
    "\n",
    "    aligned_face = cv2.warpAffine(img,M,(desired_w,desired_h))\n",
    "\n",
    "    return aligned_face\n",
    "\n",
    "def judge_eyeglass(img):\n",
    "    img = cv2.GaussianBlur(img, (11,11), 0)\n",
    "\n",
    "    sobel_y = cv2.Sobel(img, cv2.CV_64F, 0 ,1 , ksize=-1)\n",
    "    sobel_y = cv2.convertScaleAbs(sobel_y)\n",
    "    cv2.imshow('sobel_y', sobel_y)\n",
    "\n",
    "    edgeness = sobel_y\n",
    "\n",
    "    retVal, thresh = cv2.threshold(edgeness, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "    d = len(thresh) * 0.5\n",
    "    x = np.int32(d * 6/7)\n",
    "    y = np.int32(d * 3/4)\n",
    "    w = np.int32(d * 2/7)\n",
    "    h = np.int32(d * 2/4)\n",
    "\n",
    "    x_2_1 = np.int32(d * 1/4)\n",
    "    x_2_2 = np.int32(d * 5/4)\n",
    "    w_2 = np.int32(d * 1/2)\n",
    "    y_2 = np.int32(d * 8/7)\n",
    "    h_2 = np.int32(d * 1/2)\n",
    "\n",
    "    roi_1 = thresh[y:y+h, x:x+w]\n",
    "    roi_2_1 = thresh[y_2:y_2+h_2, x_2_1:x_2_1+w_2]\n",
    "    roi_2_2 = thresh[y_2:y_2+h_2, x_2_2:x_2_2+w_2]\n",
    "    roi_2 = np.hstack([roi_2_1,roi_2_2])\n",
    "\n",
    "    measure_1 = sum(sum(roi_1/255)) / (np.shape(roi_1)[0] * np.shape(roi_1)[1])\n",
    "    measure_2 = sum(sum(roi_2/255)) / (np.shape(roi_2)[0] * np.shape(roi_2)[1])\n",
    "    measure = measure_1*0.3 + measure_2*0.7\n",
    "\n",
    "    cv2.imshow('roi_1', roi_1)\n",
    "    cv2.imshow('roi_2', roi_2)\n",
    "\n",
    "    if measure > 0.15:\n",
    "        judge = True\n",
    "    else:\n",
    "        judge = False\n",
    "    return judge\n",
    "\n",
    "glasses_detected_count = 0\n",
    "for root, dirs, files in os.walk(lfw_dataset_path):\n",
    "    for file in files:\n",
    "        if file.endswith(\".jpg\"):\n",
    "            image_path = os.path.join(root, file)\n",
    "            img = cv2.imread(image_path)\n",
    "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            faces = detector(gray)\n",
    "            \n",
    "            for face in faces:\n",
    "                landmarks = predictor(gray, face)\n",
    "                landmarks = landmarks_to_np(landmarks)\n",
    "                left_eye_center, right_eye_center = get_centers(img, landmarks)\n",
    "                aligned_face = get_aligned_face(gray, left_eye_center, right_eye_center)\n",
    "                is_wearing_glasses = judge_eyeglass(aligned_face)\n",
    "\n",
    "                if is_wearing_glasses:\n",
    "                    glasses_detected_count += 1\n",
    "                    #print(f\"{image_path}: Glasses detected\")\n",
    "            \n",
    "print(f\"Total images with glasses detected: {glasses_detected_count}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CODE 3: LANDMARK POINTS, IMAGE AUGMENTATION, TRAIN USING CNN ON LFW DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading images: 5750image [00:06, 914.40image/s]s]\n",
      "Loading images: 13232image [00:06, 2047.46image/s]\n",
      "Training model:   0%|          | 0/10585 [00:00<?, ?image/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "331/331 - 38s - loss: 0.0022 - accuracy: 0.9979 - val_loss: 9.5215e-32 - val_accuracy: 1.0000 - 38s/epoch - 114ms/step\n",
      "Epoch 2/10\n",
      "331/331 - 37s - loss: 1.7746e-24 - accuracy: 1.0000 - val_loss: 9.5215e-32 - val_accuracy: 1.0000 - 37s/epoch - 112ms/step\n",
      "Epoch 3/10\n",
      "331/331 - 38s - loss: 8.0342e-26 - accuracy: 1.0000 - val_loss: 9.5215e-32 - val_accuracy: 1.0000 - 38s/epoch - 113ms/step\n",
      "Epoch 4/10\n",
      "331/331 - 38s - loss: 2.9713e-26 - accuracy: 1.0000 - val_loss: 9.5215e-32 - val_accuracy: 1.0000 - 38s/epoch - 113ms/step\n",
      "Epoch 5/10\n",
      "331/331 - 37s - loss: 5.6584e-24 - accuracy: 1.0000 - val_loss: 9.5215e-32 - val_accuracy: 1.0000 - 37s/epoch - 112ms/step\n",
      "Epoch 6/10\n",
      "331/331 - 37s - loss: 2.2016e-27 - accuracy: 1.0000 - val_loss: 9.5215e-32 - val_accuracy: 1.0000 - 37s/epoch - 113ms/step\n",
      "Epoch 7/10\n",
      "331/331 - 38s - loss: 5.7568e-26 - accuracy: 1.0000 - val_loss: 9.5215e-32 - val_accuracy: 1.0000 - 38s/epoch - 113ms/step\n",
      "Epoch 8/10\n",
      "331/331 - 37s - loss: 9.5753e-28 - accuracy: 1.0000 - val_loss: 9.5215e-32 - val_accuracy: 1.0000 - 37s/epoch - 113ms/step\n",
      "Epoch 9/10\n",
      "331/331 - 37s - loss: 1.3927e-29 - accuracy: 1.0000 - val_loss: 9.5215e-32 - val_accuracy: 1.0000 - 37s/epoch - 111ms/step\n",
      "Epoch 10/10\n",
      "331/331 - 37s - loss: 8.2102e-29 - accuracy: 1.0000 - val_loss: 9.5215e-32 - val_accuracy: 1.0000 - 37s/epoch - 112ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:   0%|          | 0/10585 [06:13<?, ?image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 2s 29ms/step - loss: 9.5215e-32 - accuracy: 1.0000\n",
      "Test Loss: 9.521530690283422e-32\n",
      "Test Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "import dlib\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "lfw_dataset_path = \"/Users/nsjain/Downloads/lfw\"\n",
    "shape_predictor_path = \"/Users/nsjain/Downloads/shape_predictor_68_face_landmarks.dat\"\n",
    "\n",
    "IMG_WIDTH, IMG_HEIGHT = 128, 128\n",
    "GLASSES_LABEL = 1\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(shape_predictor_path)\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT))\n",
    "    img = img.astype('float32') / 255.0\n",
    "    return img\n",
    "\n",
    "def extract_landmarks(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    faces = detector(gray)\n",
    "    landmarks = []\n",
    "    for face in faces:\n",
    "        shape = predictor(gray, face)\n",
    "        for i in range(68):\n",
    "            landmarks.append((shape.part(i).x, shape.part(i).y))\n",
    "    return landmarks\n",
    "\n",
    "lfw_images = []\n",
    "lfw_labels = []\n",
    "total_images = 0\n",
    "\n",
    "progress_bar = tqdm(total=total_images, desc=\"Loading images\", unit=\"image\")\n",
    "\n",
    "for root, dirs, files in os.walk(lfw_dataset_path):\n",
    "    total_images += len([file for file in files if file.endswith(\".jpg\")])\n",
    "\n",
    "for root, dirs, files in tqdm(os.walk(lfw_dataset_path), desc=\"Loading images\", unit=\"image\"):\n",
    "    for file in files:\n",
    "        if file.endswith(\".jpg\"):\n",
    "            image_path = os.path.join(root, file)\n",
    "            lfw_images.append(preprocess_image(image_path))\n",
    "            lfw_labels.append(GLASSES_LABEL if 'glasses' in file else 0)\n",
    "            progress_bar.update(1) \n",
    "\n",
    "progress_bar.close()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(lfw_images, lfw_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_WIDTH, IMG_HEIGHT, 3)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "progress_bar = tqdm(total=len(X_train), desc=\"Training model\", unit=\"image\")\n",
    "model.fit(X_train, y_train, batch_size=32, epochs=10, validation_data=(X_test, y_test), verbose=2)\n",
    "progress_bar.close() \n",
    "\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss}\")\n",
    "print(f\"Test Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 67ms/step\n",
      "No glasses detected\n"
     ]
    }
   ],
   "source": [
    "sample_image_path = \"/Users/nsjain/Downloads/lfw/Abba_Eban/Abba_Eban_0001.jpg\"\n",
    "sample_image = preprocess_image(sample_image_path)\n",
    "\n",
    "sample_image = np.expand_dims(sample_image, axis=0)\n",
    "\n",
    "predictions = model.predict(sample_image)\n",
    "\n",
    "if predictions[0] >= 0.5:\n",
    "    print(\"Glasses detected\")\n",
    "else:\n",
    "    print(\"No glasses detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 17ms/step\n",
      "No glasses detected\n"
     ]
    }
   ],
   "source": [
    "sample_image_path = \"/Users/nsjain/Downloads/lfw/Aaron_Sorkin/Aaron_Sorkin_0001.jpg\"\n",
    "sample_image = preprocess_image(sample_image_path)\n",
    "\n",
    "sample_image = np.expand_dims(sample_image, axis=0)\n",
    "\n",
    "predictions = model.predict(sample_image)\n",
    "\n",
    "if predictions[0] >= 0.5:\n",
    "    print(\"Glasses detected\")\n",
    "else:\n",
    "    print(\"No glasses detected\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CODE 4: CREATED TWO DATASETS OF GLASSES AND NON GLASSES AND IMPLEMENTED THE SAME CODE AS ABOVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "158/158 [==============================] - 18s 111ms/step - loss: 0.5635 - accuracy: 0.7544 - val_loss: 0.5466 - val_accuracy: 0.7441\n",
      "Epoch 2/10\n",
      "158/158 [==============================] - 17s 109ms/step - loss: 0.5197 - accuracy: 0.7573 - val_loss: 0.4954 - val_accuracy: 0.7583\n",
      "Epoch 3/10\n",
      "158/158 [==============================] - 17s 110ms/step - loss: 0.4754 - accuracy: 0.7722 - val_loss: 0.5032 - val_accuracy: 0.7867\n",
      "Epoch 4/10\n",
      "158/158 [==============================] - 17s 108ms/step - loss: 0.4518 - accuracy: 0.7833 - val_loss: 0.4911 - val_accuracy: 0.7820\n",
      "Epoch 5/10\n",
      "158/158 [==============================] - 17s 107ms/step - loss: 0.4164 - accuracy: 0.8021 - val_loss: 0.4706 - val_accuracy: 0.7820\n",
      "Epoch 6/10\n",
      "158/158 [==============================] - 17s 107ms/step - loss: 0.3766 - accuracy: 0.8197 - val_loss: 0.5068 - val_accuracy: 0.7820\n",
      "Epoch 7/10\n",
      "158/158 [==============================] - 17s 107ms/step - loss: 0.3495 - accuracy: 0.8385 - val_loss: 0.5237 - val_accuracy: 0.7962\n",
      "Epoch 8/10\n",
      "158/158 [==============================] - 17s 109ms/step - loss: 0.2888 - accuracy: 0.8654 - val_loss: 0.5529 - val_accuracy: 0.7915\n",
      "Epoch 9/10\n",
      "158/158 [==============================] - 17s 109ms/step - loss: 0.2377 - accuracy: 0.8973 - val_loss: 0.5804 - val_accuracy: 0.7962\n",
      "Epoch 10/10\n",
      "158/158 [==============================] - 17s 108ms/step - loss: 0.2029 - accuracy: 0.9093 - val_loss: 0.6809 - val_accuracy: 0.8152\n",
      "7/7 [==============================] - 0s 33ms/step - loss: 0.6809 - accuracy: 0.8152\n",
      "Test Loss: 0.6808797121047974\n",
      "Test Accuracy: 0.8151658773422241\n"
     ]
    }
   ],
   "source": [
    "import dlib\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "noglasses_dataset_path = \"/Users/nsjain/Desktop/noglasses\"\n",
    "shape_predictor_path = \"/Users/nsjain/Downloads/shape_predictor_68_face_landmarks.dat\"\n",
    "glasses_images_path = \"/Users/nsjain/Desktop/glasses\"\n",
    "\n",
    "IMG_WIDTH, IMG_HEIGHT = 128, 128\n",
    "GLASSES_LABEL = 1\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(shape_predictor_path)\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT))\n",
    "    img = img.astype('float32') / 255.0\n",
    "    return img\n",
    "\n",
    "def extract_landmarks(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    faces = detector(gray)\n",
    "    landmarks = []\n",
    "    for face in faces:\n",
    "        shape = predictor(gray, face)\n",
    "        for i in range(68):\n",
    "            landmarks.append((shape.part(i).x, shape.part(i).y))\n",
    "    return landmarks\n",
    "\n",
    "def generate_augmented_images(images, labels):\n",
    "    datagen = ImageDataGenerator(\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        zoom_range=0.1,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "    augmented_images = []\n",
    "    augmented_labels = []\n",
    "    for image, label in zip(images, labels):\n",
    "        image = np.expand_dims(image, axis=0)\n",
    "        aug_iter = datagen.flow(image, batch_size=1)\n",
    "        for _ in range(5): \n",
    "            augmented_image = aug_iter.next()[0]\n",
    "            augmented_images.append(augmented_image)\n",
    "            augmented_labels.append(label)\n",
    "    return np.array(augmented_images), np.array(augmented_labels)\n",
    "\n",
    "glasses_images = []\n",
    "glasses_labels = []\n",
    "for root, dirs, files in os.walk(glasses_images_path):\n",
    "    for file in files:\n",
    "        if file.endswith(\".jpg\"):\n",
    "            image_path = os.path.join(root, file)\n",
    "            glasses_images.append(preprocess_image(image_path))\n",
    "            glasses_labels.append(GLASSES_LABEL)\n",
    "\n",
    "lfw_images = []\n",
    "lfw_labels = []\n",
    "for root, dirs, files in os.walk(lfw_dataset_path):\n",
    "    for file in files:\n",
    "        if file.endswith(\".jpg\"):\n",
    "            image_path = os.path.join(root, file)\n",
    "            lfw_images.append(preprocess_image(image_path))\n",
    "            lfw_labels.append(0)\n",
    "\n",
    "images = glasses_images + lfw_images\n",
    "labels = glasses_labels + lfw_labels\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "X_train_aug, y_train_aug = generate_augmented_images(X_train, y_train)\n",
    "\n",
    "X_train_final = np.concatenate((X_train, X_train_aug), axis=0)\n",
    "y_train_final = np.concatenate((y_train, y_train_aug), axis=0)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_WIDTH, IMG_HEIGHT, 3)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "model.fit(X_train_final, y_train_final, batch_size=32, epochs=10, validation_data=(X_test, y_test))\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss}\")\n",
    "print(f\"Test Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 27ms/step\n",
      "Confusion Matrix:\n",
      "[[149   8]\n",
      " [ 31  23]]\n",
      "EER: 0.3769, FAR: 1.0000, FRR: 0.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from scipy.optimize import brentq\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = (y_pred > 0.5).astype(int)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred)\n",
    "\n",
    "eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
    "print(f\"Equal Error Rate (EER): {eer}\")\n",
    "print(\"False Positive Rate:\", fpr)\n",
    "print(\"True Positive Rate:\", tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Images: 260\n",
      "Number of Labels: 260\n"
     ]
    }
   ],
   "source": [
    "#With glasses dataset\n",
    "num_images = len(glasses_images)\n",
    "num_labels = len(glasses_labels)\n",
    "print(\"Number of Images:\", num_images)\n",
    "print(\"Number of Labels:\", num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 27ms/step\n",
      "9/9 [==============================] - 0s 30ms/step - loss: 0.7190 - accuracy: 0.7769\n",
      "Test Loss: 0.7189658880233765\n",
      "Test Accuracy: 0.7769230604171753\n",
      "Confusion Matrix:\n",
      "[[  0   0]\n",
      " [ 58 202]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_ranking.py:1123: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\neer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\\nfar = fpr[np.nanargmin(np.abs(fpr - 1. - eer))]\\nfrr = 1 - tpr[np.nanargmin(np.abs(fpr - 1. - eer))]\\nprint(f'EER: {eer:.4f}, FAR: {far:.4f}, FRR: {frr:.4f}')\\n\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = np.array(glasses_images)\n",
    "y_test = np.array(glasses_labels)\n",
    "\n",
    "# Perform any required data transformations or scaling on X_test\n",
    "\n",
    "# Make predictions using your model\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = (y_pred > 0.5).astype(int)\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss}\")\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "\n",
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Calculate EER, FAR, and FRR\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred)\n",
    "\n",
    "\"\"\"\n",
    "eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
    "far = fpr[np.nanargmin(np.abs(fpr - 1. - eer))]\n",
    "frr = 1 - tpr[np.nanargmin(np.abs(fpr - 1. - eer))]\n",
    "print(f'EER: {eer:.4f}, FAR: {far:.4f}, FRR: {frr:.4f}')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Images: 793\n",
      "Number of Labels: 793\n"
     ]
    }
   ],
   "source": [
    "#Without glasses dataset\n",
    "lfw_images = len(lfw_images)\n",
    "lfw_labels = len(lfw_labels)\n",
    "print(\"Number of Images:\", lfw_images)\n",
    "print(\"Number of Labels:\", lfw_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 7\u001b[0m\n\u001b[1;32m      2\u001b[0m y_test \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(lfw_labels)\n\u001b[1;32m      4\u001b[0m \u001b[39m# Perform any required data transformations or scaling on X_test\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m \u001b[39m# Make predictions using your model\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m y_pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(X_test)\n\u001b[1;32m      8\u001b[0m y_pred \u001b[39m=\u001b[39m (y_pred \u001b[39m>\u001b[39m \u001b[39m0.5\u001b[39m)\u001b[39m.\u001b[39mastype(\u001b[39mint\u001b[39m)\n\u001b[1;32m     10\u001b[0m loss, accuracy \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mevaluate(X_test, y_test)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/framework/tensor_shape.py:959\u001b[0m, in \u001b[0;36mTensorShape.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    957\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    958\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_v2_behavior:\n\u001b[0;32m--> 959\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dims[key]\n\u001b[1;32m    960\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    961\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdims[key]\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "X_test = np.array(lfw_images)\n",
    "y_test = np.array(lfw_labels)\n",
    "\n",
    "# Perform any required data transformations or scaling on X_test\n",
    "\n",
    "# Make predictions using your model\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = (y_pred > 0.5).astype(int)\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss}\")\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "\n",
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Calculate EER, FAR, and FRR\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred)\n",
    "\n",
    "\"\"\"\n",
    "eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
    "far = fpr[np.nanargmin(np.abs(fpr - 1. - eer))]\n",
    "frr = 1 - tpr[np.nanargmin(np.abs(fpr - 1. - eer))]\n",
    "print(f'EER: {eer:.4f}, FAR: {far:.4f}, FRR: {frr:.4f}')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 42ms/step\n",
      "Glasses detected\n"
     ]
    }
   ],
   "source": [
    "sample_image_path = \"/Users/nsjain/Downloads/lfw/Abba_Eban/Abba_Eban_0001.jpg\"\n",
    "sample_image = preprocess_image(sample_image_path)\n",
    "\n",
    "sample_image = np.expand_dims(sample_image, axis=0)\n",
    "\n",
    "predictions = model.predict(sample_image)\n",
    "\n",
    "if predictions[0] >= 0.5:\n",
    "    print(\"Glasses detected\")\n",
    "else:\n",
    "    print(\"No glasses detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step\n",
      "No glasses detected\n"
     ]
    }
   ],
   "source": [
    "sample_image_path = \"/Users/nsjain/Downloads/lfw/Aaron_Sorkin/Aaron_Sorkin_0001.jpg\"\n",
    "sample_image = preprocess_image(sample_image_path)\n",
    "\n",
    "sample_image = np.expand_dims(sample_image, axis=0)\n",
    "\n",
    "predictions = model.predict(sample_image)\n",
    "\n",
    "if predictions[0] >= 0.5:\n",
    "    print(\"Glasses detected\")\n",
    "else:\n",
    "    print(\"No glasses detected\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CODE 5: AMAZON REKOGNITION API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NoCredentialsError",
     "evalue": "Unable to locate credentials",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoCredentialsError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m     image_bytes \u001b[39m=\u001b[39m image_file\u001b[39m.\u001b[39mread()\n\u001b[1;32m     21\u001b[0m \u001b[39m# Call the DetectFaces API to detect facial features\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m response \u001b[39m=\u001b[39m rekognition_client\u001b[39m.\u001b[39;49mdetect_faces(\n\u001b[1;32m     23\u001b[0m     Image\u001b[39m=\u001b[39;49m{\u001b[39m'\u001b[39;49m\u001b[39mBytes\u001b[39;49m\u001b[39m'\u001b[39;49m: image_bytes},\n\u001b[1;32m     24\u001b[0m     Attributes\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mALL\u001b[39;49m\u001b[39m'\u001b[39;49m]  \u001b[39m# Specify additional face attributes to retrieve if needed\u001b[39;49;00m\n\u001b[1;32m     25\u001b[0m )\n\u001b[1;32m     27\u001b[0m \u001b[39m# Check if eyeglasses are detected for each detected face\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[39mfor\u001b[39;00m face_detail \u001b[39min\u001b[39;00m response[\u001b[39m'\u001b[39m\u001b[39mFaceDetails\u001b[39m\u001b[39m'\u001b[39m]:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/botocore/client.py:534\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    531\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpy_operation_name\u001b[39m}\u001b[39;00m\u001b[39m() only accepts keyword arguments.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    532\u001b[0m     )\n\u001b[1;32m    533\u001b[0m \u001b[39m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 534\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_api_call(operation_name, kwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/botocore/client.py:959\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    957\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    958\u001b[0m     apply_request_checksum(request_dict)\n\u001b[0;32m--> 959\u001b[0m     http, parsed_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    960\u001b[0m         operation_model, request_dict, request_context\n\u001b[1;32m    961\u001b[0m     )\n\u001b[1;32m    963\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmeta\u001b[39m.\u001b[39mevents\u001b[39m.\u001b[39memit(\n\u001b[1;32m    964\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mafter-call.\u001b[39m\u001b[39m{service_id}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{operation_name}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    965\u001b[0m         service_id\u001b[39m=\u001b[39mservice_id, operation_name\u001b[39m=\u001b[39moperation_name\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    970\u001b[0m     context\u001b[39m=\u001b[39mrequest_context,\n\u001b[1;32m    971\u001b[0m )\n\u001b[1;32m    973\u001b[0m \u001b[39mif\u001b[39;00m http\u001b[39m.\u001b[39mstatus_code \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m300\u001b[39m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/botocore/client.py:982\u001b[0m, in \u001b[0;36mBaseClient._make_request\u001b[0;34m(self, operation_model, request_dict, request_context)\u001b[0m\n\u001b[1;32m    980\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_make_request\u001b[39m(\u001b[39mself\u001b[39m, operation_model, request_dict, request_context):\n\u001b[1;32m    981\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 982\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_endpoint\u001b[39m.\u001b[39;49mmake_request(operation_model, request_dict)\n\u001b[1;32m    983\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    984\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmeta\u001b[39m.\u001b[39mevents\u001b[39m.\u001b[39memit(\n\u001b[1;32m    985\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mafter-call-error.\u001b[39m\u001b[39m{service_id}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{operation_name}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    986\u001b[0m                 service_id\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_service_model\u001b[39m.\u001b[39mservice_id\u001b[39m.\u001b[39mhyphenize(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    990\u001b[0m             context\u001b[39m=\u001b[39mrequest_context,\n\u001b[1;32m    991\u001b[0m         )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/botocore/endpoint.py:119\u001b[0m, in \u001b[0;36mEndpoint.make_request\u001b[0;34m(self, operation_model, request_dict)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmake_request\u001b[39m(\u001b[39mself\u001b[39m, operation_model, request_dict):\n\u001b[1;32m    114\u001b[0m     logger\u001b[39m.\u001b[39mdebug(\n\u001b[1;32m    115\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMaking request for \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m with params: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    116\u001b[0m         operation_model,\n\u001b[1;32m    117\u001b[0m         request_dict,\n\u001b[1;32m    118\u001b[0m     )\n\u001b[0;32m--> 119\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_request(request_dict, operation_model)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/botocore/endpoint.py:198\u001b[0m, in \u001b[0;36mEndpoint._send_request\u001b[0;34m(self, request_dict, operation_model)\u001b[0m\n\u001b[1;32m    196\u001b[0m context \u001b[39m=\u001b[39m request_dict[\u001b[39m'\u001b[39m\u001b[39mcontext\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    197\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_retries_context(context, attempts)\n\u001b[0;32m--> 198\u001b[0m request \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcreate_request(request_dict, operation_model)\n\u001b[1;32m    199\u001b[0m success_response, exception \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_response(\n\u001b[1;32m    200\u001b[0m     request, operation_model, context\n\u001b[1;32m    201\u001b[0m )\n\u001b[1;32m    202\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_needs_retry(\n\u001b[1;32m    203\u001b[0m     attempts,\n\u001b[1;32m    204\u001b[0m     operation_model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    207\u001b[0m     exception,\n\u001b[1;32m    208\u001b[0m ):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/botocore/endpoint.py:134\u001b[0m, in \u001b[0;36mEndpoint.create_request\u001b[0;34m(self, params, operation_model)\u001b[0m\n\u001b[1;32m    130\u001b[0m     service_id \u001b[39m=\u001b[39m operation_model\u001b[39m.\u001b[39mservice_model\u001b[39m.\u001b[39mservice_id\u001b[39m.\u001b[39mhyphenize()\n\u001b[1;32m    131\u001b[0m     event_name \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mrequest-created.\u001b[39m\u001b[39m{service_id}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{op_name}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    132\u001b[0m         service_id\u001b[39m=\u001b[39mservice_id, op_name\u001b[39m=\u001b[39moperation_model\u001b[39m.\u001b[39mname\n\u001b[1;32m    133\u001b[0m     )\n\u001b[0;32m--> 134\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_event_emitter\u001b[39m.\u001b[39;49memit(\n\u001b[1;32m    135\u001b[0m         event_name,\n\u001b[1;32m    136\u001b[0m         request\u001b[39m=\u001b[39;49mrequest,\n\u001b[1;32m    137\u001b[0m         operation_name\u001b[39m=\u001b[39;49moperation_model\u001b[39m.\u001b[39;49mname,\n\u001b[1;32m    138\u001b[0m     )\n\u001b[1;32m    139\u001b[0m prepared_request \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_request(request)\n\u001b[1;32m    140\u001b[0m \u001b[39mreturn\u001b[39;00m prepared_request\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/botocore/hooks.py:412\u001b[0m, in \u001b[0;36mEventAliaser.emit\u001b[0;34m(self, event_name, **kwargs)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39memit\u001b[39m(\u001b[39mself\u001b[39m, event_name, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    411\u001b[0m     aliased_event_name \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_alias_event_name(event_name)\n\u001b[0;32m--> 412\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_emitter\u001b[39m.\u001b[39;49memit(aliased_event_name, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/botocore/hooks.py:256\u001b[0m, in \u001b[0;36mHierarchicalEmitter.emit\u001b[0;34m(self, event_name, **kwargs)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39memit\u001b[39m(\u001b[39mself\u001b[39m, event_name, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    246\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \u001b[39m    Emit an event by name with arguments passed as keyword args.\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[39m             handlers.\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 256\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_emit(event_name, kwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/botocore/hooks.py:239\u001b[0m, in \u001b[0;36mHierarchicalEmitter._emit\u001b[0;34m(self, event_name, kwargs, stop_on_response)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[39mfor\u001b[39;00m handler \u001b[39min\u001b[39;00m handlers_to_call:\n\u001b[1;32m    238\u001b[0m     logger\u001b[39m.\u001b[39mdebug(\u001b[39m'\u001b[39m\u001b[39mEvent \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m: calling handler \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m, event_name, handler)\n\u001b[0;32m--> 239\u001b[0m     response \u001b[39m=\u001b[39m handler(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    240\u001b[0m     responses\u001b[39m.\u001b[39mappend((handler, response))\n\u001b[1;32m    241\u001b[0m     \u001b[39mif\u001b[39;00m stop_on_response \u001b[39mand\u001b[39;00m response \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/botocore/signers.py:105\u001b[0m, in \u001b[0;36mRequestSigner.handler\u001b[0;34m(self, operation_name, request, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhandler\u001b[39m(\u001b[39mself\u001b[39m, operation_name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, request\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    101\u001b[0m     \u001b[39m# This is typically hooked up to the \"request-created\" event\u001b[39;00m\n\u001b[1;32m    102\u001b[0m     \u001b[39m# from a client's event emitter.  When a new request is created\u001b[39;00m\n\u001b[1;32m    103\u001b[0m     \u001b[39m# this method is invoked to sign the request.\u001b[39;00m\n\u001b[1;32m    104\u001b[0m     \u001b[39m# Don't call this method directly.\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msign(operation_name, request)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/botocore/signers.py:189\u001b[0m, in \u001b[0;36mRequestSigner.sign\u001b[0;34m(self, operation_name, request, region_name, signing_type, expires_in, signing_name)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m         \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m--> 189\u001b[0m auth\u001b[39m.\u001b[39;49madd_auth(request)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/botocore/auth.py:418\u001b[0m, in \u001b[0;36mSigV4Auth.add_auth\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39madd_auth\u001b[39m(\u001b[39mself\u001b[39m, request):\n\u001b[1;32m    417\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcredentials \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 418\u001b[0m         \u001b[39mraise\u001b[39;00m NoCredentialsError()\n\u001b[1;32m    419\u001b[0m     datetime_now \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mdatetime\u001b[39m.\u001b[39mutcnow()\n\u001b[1;32m    420\u001b[0m     request\u001b[39m.\u001b[39mcontext[\u001b[39m'\u001b[39m\u001b[39mtimestamp\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m datetime_now\u001b[39m.\u001b[39mstrftime(SIGV4_TIMESTAMP)\n",
      "\u001b[0;31mNoCredentialsError\u001b[0m: Unable to locate credentials"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "rekognition_client = boto3.client('rekognition', region_name='us-east-1')\n",
    "\n",
    "import os\n",
    "\n",
    "lfw_dataset_path = \"/Users/nsjain/Downloads/lfw\"\n",
    "\n",
    "for root, dirs, files in os.walk(lfw_dataset_path):\n",
    "    for file in tqdm(files, desc=\"Processing files\", unit=\"file\"):\n",
    "        if file.endswith(\".jpg\"):\n",
    "            image_path = os.path.join(root, file)\n",
    "            img = Image.open(image_path)\n",
    "            img = img.resize((128, 128))\n",
    "            if img.mode != \"RGB\":\n",
    "                img = img.convert(\"RGB\")\n",
    "\n",
    "            with io.BytesIO() as output:\n",
    "                img.save(output, format=\"JPEG\")\n",
    "                image_bytes = output.getvalue()\n",
    "\n",
    "            response = rekognition_client.detect_faces(\n",
    "                Image={'Bytes': image_bytes},\n",
    "                Attributes=['ALL']\n",
    "            )\n",
    "            \n",
    "            for face_detail in response['FaceDetails']:\n",
    "                if face_detail['Eyeglasses']['Value']:\n",
    "                    print(f\"Glasses detected in {image_path}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OPENCV HAAR CASCADE PRE_TRAINED CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images with detected glasses: 91\n",
      "Total images with glasses: 260\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "eye_cascade = cv2.CascadeClassifier('/Users/nsjain/Downloads/haarcascade_eye.xml')\n",
    "glasses_cascade = cv2.CascadeClassifier('/Users/nsjain/Downloads/haarcascade_eye_tree_eyeglasses.xml')\n",
    "\n",
    "lfw_path = '/Users/nsjain/Desktop/glasses'\n",
    "\n",
    "glasses_count = 0\n",
    "image_count = 0\n",
    "\n",
    "for root, dirs, files in os.walk(lfw_path):\n",
    "    for file in files:\n",
    "        if file.endswith(\".jpg\"):\n",
    "            image_count += 1\n",
    "            image_path = os.path.join(root, file)\n",
    "\n",
    "            image = cv2.imread(image_path)\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            eyes = eye_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "            for (ex, ey, ew, eh) in eyes:\n",
    "                roi_gray = gray[ey:ey+eh, ex:ex+ew]\n",
    "\n",
    "                # Perform glasses detection within the eye region\n",
    "                glasses = glasses_cascade.detectMultiScale(roi_gray, scaleFactor=1.05, minNeighbors=3, minSize=(10, 10))\n",
    "\n",
    "                if len(glasses) > 0:\n",
    "                    glasses_count += 1\n",
    "\n",
    "        \"\"\"\n",
    "        # Draw rectangles around the detected eyes\n",
    "        for (x, y, w, h) in eyes:\n",
    "            cv2.rectangle(image, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "        \n",
    "        # Draw rectangles around the detected eyeglasses\n",
    "        for (x, y, w, h) in glasses:\n",
    "            cv2.rectangle(image, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "        \n",
    "        # Display the image with the detections\n",
    "        cv2.imshow('Image', image)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "        \"\"\"\n",
    "        \n",
    "print(f\"Number of images with detected glasses: {glasses_count}\")\n",
    "print(\"Total images with glasses:\", image_count)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TENSORFLOW OBJECT DETECTION API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as vis_util\n",
    "import cv2\n",
    "\n",
    "# Path to the frozen inference graph and label map\n",
    "PATH_TO_FROZEN_GRAPH = '/path/to/frozen_inference_graph.pb'\n",
    "PATH_TO_LABEL_MAP = '/path/to/label_map.pbtxt'\n",
    "\n",
    "# Load the frozen inference graph\n",
    "detection_graph = tf.Graph()\n",
    "with detection_graph.as_default():\n",
    "    od_graph_def = tf.compat.v1.GraphDef()\n",
    "    with tf.io.gfile.GFile(PATH_TO_FROZEN_GRAPH, 'rb') as fid:\n",
    "        serialized_graph = fid.read()\n",
    "        od_graph_def.ParseFromString(serialized_graph)\n",
    "        tf.import_graph_def(od_graph_def, name='')\n",
    "\n",
    "# Load the label map\n",
    "label_map = label_map_util.load_labelmap(PATH_TO_LABEL_MAP)\n",
    "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=2, use_display_name=True)\n",
    "category_index = label_map_util.create_category_index(categories)\n",
    "\n",
    "# Function to perform object detection on an image\n",
    "def detect_glasses(image):\n",
    "    with detection_graph.as_default():\n",
    "        with tf.compat.v1.Session(graph=detection_graph) as sess:\n",
    "            # Preprocess the image\n",
    "            image_np_expanded = np.expand_dims(image, axis=0)\n",
    "            image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
    "            boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n",
    "            scores = detection_graph.get_tensor_by_name('detection_scores:0')\n",
    "            classes = detection_graph.get_tensor_by_name('detection_classes:0')\n",
    "            num_detections = detection_graph.get_tensor_by_name('num_detections:0')\n",
    "\n",
    "            # Perform inference\n",
    "            (boxes, scores, classes, num_detections) = sess.run(\n",
    "                [boxes, scores, classes, num_detections],\n",
    "                feed_dict={image_tensor: image_np_expanded})\n",
    "\n",
    "            # Visualize the results\n",
    "            vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "                image,\n",
    "                np.squeeze(boxes),\n",
    "                np.squeeze(classes).astype(np.int32),\n",
    "                np.squeeze(scores),\n",
    "                category_index,\n",
    "                use_normalized_coordinates=True,\n",
    "                line_thickness=4,\n",
    "                min_score_thresh=0.5)\n",
    "\n",
    "            return image\n",
    "\n",
    "# Path to your dataset directory\n",
    "dataset_path = '/path/to/dataset'\n",
    "\n",
    "glasses_detected_count = 0\n",
    "total_images = 0\n",
    "\n",
    "# Process each image in the dataset\n",
    "for root, dirs, files in os.walk(dataset_path):\n",
    "    for file in files:\n",
    "        if file.endswith('.jpg'):\n",
    "            total_images += 1\n",
    "            image_path = os.path.join(root, file)\n",
    "            image = cv2.imread(image_path)\n",
    "            image_with_glasses = detect_glasses(image)\n",
    "\n",
    "            # Count the number of images with detected glasses\n",
    "            if np.any(image_with_glasses):\n",
    "                glasses_detected_count += 1\n",
    "\n",
    "# Print the results\n",
    "print(\"Number of images with detected glasses:\", glasses_detected_count)\n",
    "print(\"Total images with glasses:\", total_images)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INSIGHTFACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /Users/nsjain/.insightface/models/buffalo_l/1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /Users/nsjain/.insightface/models/buffalo_l/2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /Users/nsjain/.insightface/models/buffalo_l/det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /Users/nsjain/.insightface/models/buffalo_l/genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /Users/nsjain/.insightface/models/buffalo_l/w600k_r50.onnx recognition ['None', 3, 112, 112] 127.5 127.5\n",
      "set det-size: (640, 640)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'insightface.app' has no attribute 'FaceDetector'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m model \u001b[39m=\u001b[39m FaceAnalysis()\n\u001b[1;32m      5\u001b[0m model\u001b[39m.\u001b[39mprepare(ctx_id\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m detector \u001b[39m=\u001b[39m insightface\u001b[39m.\u001b[39;49mapp\u001b[39m.\u001b[39;49mFaceDetector()\n\u001b[1;32m      8\u001b[0m detector\u001b[39m.\u001b[39mprepare(ctx_id\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, nms\u001b[39m=\u001b[39m\u001b[39m0.4\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39malign_face\u001b[39m(image, bbox):\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'insightface.app' has no attribute 'FaceDetector'"
     ]
    }
   ],
   "source": [
    "import insightface\n",
    "from insightface.app import FaceAnalysis\n",
    "\n",
    "model = FaceAnalysis()\n",
    "model.prepare(ctx_id=-1)\n",
    "\n",
    "detector = insightface.app.FaceDetector()\n",
    "detector.prepare(ctx_id=-1, nms=0.4)\n",
    "\n",
    "def align_face(image, bbox):\n",
    "    landmarks = model.get_landmarks(image, bbox)\n",
    "    return insightface.utils.face_align.norm_crop(image, landmarks)\n",
    "\n",
    "def detect_glasses(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    faces = model.get(image)\n",
    "    \n",
    "    glasses_count = 0\n",
    "    \n",
    "    for idx, face in enumerate(faces):\n",
    "        if face.landmark is not None:\n",
    "            landmarks = face.landmark.astype(int)\n",
    "            \n",
    "            left_eye = landmarks[36:42]\n",
    "            right_eye = landmarks[42:48]\n",
    "            \n",
    "            avg_left_eye = np.mean(left_eye, axis=0)\n",
    "            avg_right_eye = np.mean(right_eye, axis=0)\n",
    "            \n",
    "            distance = np.linalg.norm(avg_left_eye - avg_right_eye)\n",
    "            if distance > 10:\n",
    "                glasses_count += 1\n",
    "    \n",
    "    return glasses_count\n",
    "\n",
    "\n",
    "lfw_dataset_path = \"//Users/nsjain/Desktop/glasses\"\n",
    "\n",
    "glasses_detected_count = 0\n",
    "total_images = 0\n",
    "\n",
    "for root, dirs, files in os.walk(lfw_dataset_path):\n",
    "    for file in files:\n",
    "        if file.endswith(\".jpg\"):\n",
    "            total_images += 1\n",
    "            image_path = os.path.join(root, file)\n",
    "            \n",
    "            glasses_count = detect_glasses(image_path)\n",
    "            \n",
    "            if glasses_count > 0:\n",
    "                glasses_detected_count += 1\n",
    "\n",
    "print(\"Number of images with detected glasses:\", glasses_detected_count)\n",
    "print(\"Total images with glasses:\", total_images)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
